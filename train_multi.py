import datetime
import os

import torch
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import VecNormalize, VecCheckNan

from CarGameEnv import CarGameEnv


class TrainingConfig:
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    STAGES = {
        "initial": {
            "total_timesteps": 1_000_000,
            "n_envs": 12,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º GPU
            "n_steps": 8192,  # –ö—Ä—É–ø–Ω—ã–µ –±–∞—Ç—á–∏ –¥–ª—è GPU
            "batch_size": 512,  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –ø–∞–º—è—Ç–∏ GPU
            "learning_rate": 2.5e-4,
            "ent_coef": 0.3,
            "gamma": 0.99,
            "clip_range": 0.3,
            "network_size": [512, 512],
        },
        "medium": {
            "total_timesteps": 3_000_000,
            "n_envs": 24,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
            "n_steps": 4096,
            "batch_size": 1024,
            "learning_rate": 1e-4,
            "ent_coef": 0.1,
            "gamma": 0.995,
            "clip_range": 0.2,
            "network_size": [512, 512, 256],
        },
        "final": {
            "total_timesteps": 4_000_000,
            "n_envs": 24,
            "n_steps": 2048,
            "batch_size": 2048,
            "learning_rate": 5e-5,
            "ent_coef": 0.01,
            "gamma": 0.998,
            "clip_range": 0.1,
            "network_size": [1024, 512, 256],
        },
        "expert": {
            "total_timesteps": 5_000_000,
            "n_envs": 24,
            "n_steps": 2048,
            "batch_size": 2048,
            "learning_rate": 1e-5,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π LR
            "ent_coef": 0.001,  # –ú–µ–Ω—å—à–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            "gamma": 0.999,
            "clip_range": 0.05,  # –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞
            "network_size": [1024, 512, 256, 128],
        },
        "master": {
            "total_timesteps": 10_000_000,
            "n_envs": 32,
            "n_steps": 4096,
            "batch_size": 4096,
            "learning_rate": 5e-6,
            "ent_coef": 0.0001,
            "gamma": 0.9995,
            "clip_range": 0.02,
            "network_size": [2048, 1024, 512],
        },
    }


def train(stage_name, model=None, vec_normalize=None):
    config = TrainingConfig.STAGES[stage_name]

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–µ–¥—ã
        env = make_vec_env(
            lambda: CarGameEnv(track_name="track2", render_mode=None),
            n_envs=config["n_envs"],
        )
        env = VecCheckNan(env)

        # –ë–ª–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        vecnorm_path = "v1/vecnormalize.pkl"
        if os.path.exists(vecnorm_path) and vec_normalize is None:
            env = VecNormalize.load(vecnorm_path, env)
            print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ vecnormalize.pkl")
        else:
            env = VecNormalize(env, norm_obs=True, norm_reward=True, training=True)

        # –ë–ª–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        model_path = "v1/latest.zip"
        if model is None and os.path.exists(model_path):
            model = PPO.load(model_path, env=env, device="cuda")
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –∏–∑ {model_path}")

        if model is None:
            policy_kwargs = dict(
                net_arch=dict(pi=config["network_size"], vf=config["network_size"]),
                activation_fn=torch.nn.ReLU,
                ortho_init=True,
            )

            model = PPO(
                "MlpPolicy",
                env,
                verbose=2,
                device="cuda",
                n_steps=config["n_steps"],
                batch_size=config["batch_size"],
                learning_rate=config["learning_rate"],
                ent_coef=config["ent_coef"],
                gamma=config["gamma"],
                clip_range=config["clip_range"],
                vf_coef=0.5,
                max_grad_norm=0.7,
                policy_kwargs=policy_kwargs,
            )

        # –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è
        stage_start = datetime.datetime.now()
        print(f"\n‚è≥ –ù–∞—á–∞–ª–æ —ç—Ç–∞–ø–∞ '{stage_name}' –≤ {stage_start:%Y-%m-%d %H:%M:%S}")

        try:
            model.learn(
                total_timesteps=config["total_timesteps"],
                callback=EvalCallback(
                    eval_env=env,
                    eval_freq=config["total_timesteps"] // 40,
                    best_model_save_path="./models/best",
                ),
                reset_num_timesteps=False,
            )

        except Exception as e:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
            error_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            error_model_path = f"./models/crash_{stage_name}_{error_time}.zip"
            error_vecnorm_path = f"./models/crash_vecnorm_{error_time}.pkl"

            print(f"\n‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ '{stage_name}': {str(e)}")
            print(f"üíæ –≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ {error_model_path}")
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ {error_vecnorm_path}")

            model.save(error_model_path)
            env.save(error_vecnorm_path)
            raise  # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–∞–ª—å—à–µ

        finally:
            # –§–∏–Ω–∞–ª–∏–∑–∏—Ä—É—é—â–∞—è —á–∞—Å—Ç—å –≤—Å–µ–≥–¥–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è
            stage_end = datetime.datetime.now()
            duration = stage_end - stage_start
            print(f"\n‚úÖ –≠—Ç–∞–ø '{stage_name}' –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {duration}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            model.save("./models/latest")
            env.save("./models/vecnormalize.pkl")
            print(f"üíæ –ú–æ–¥–µ–ª—å –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ models")

    except Exception as e:
        # –ì–ª–æ–±–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
        print(f"\nüî• –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        print("üîÑ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å n_envs –∏–ª–∏ batch_size")
        raise

    return model, env


if __name__ == "__main__":
    # –≠—Ç–∞–ø—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π
    model, vec_norm = train("initial")
    model, vec_norm = train("medium", model, vec_norm)
    model, vec_norm = train("final", model, vec_norm)
    model, vec_norm = train("expert", model, vec_norm)
    model, vec_norm = train("master", model, vec_norm)
