import datetime
import os

import torch
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import VecNormalize, VecCheckNan

from CarGameEnv import CarGameEnv
from callbacks import (
    ProgressCurriculumCallback,
    TargetedExploration,
    AdaptiveCurriculumCallback,
)


class TrainingConfig:
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    STAGES = {
        "initial": {
            "total_timesteps": 1_000_000,
            "n_envs": 12,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º GPU
            "n_steps": 8192,  # –ö—Ä—É–ø–Ω—ã–µ –±–∞—Ç—á–∏ –¥–ª—è GPU
            "batch_size": 512,  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –ø–∞–º—è—Ç–∏ GPU
            "learning_rate": 2.5e-4,
            "ent_coef": 0.3,
            "gamma": 0.99,
            "clip_range": 0.3,
            "network_size": [512, 512],
            "reward_tuning": {},
            "focus_areas": [],
            "network_migration": {},
        },
        "medium": {
            "total_timesteps": 3_000_000,
            "n_envs": 24,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
            "n_steps": 4096,
            "batch_size": 1024,
            "learning_rate": 1e-4,
            "ent_coef": 0.1,
            "gamma": 0.995,
            "clip_range": 0.2,
            "network_size": [512, 512, 256],
            "reward_tuning": {},
            "focus_areas": [],
            "network_migration": {},
        },
        "final": {
            "total_timesteps": 4_000_000,
            "n_envs": 24,
            "n_steps": 2048,
            "batch_size": 2048,
            "learning_rate": 5e-5,
            "ent_coef": 0.01,
            "gamma": 0.998,
            "clip_range": 0.1,
            "network_size": [1024, 512, 256],
            "reward_tuning": {},
            "focus_areas": [],
            "network_migration": {},
        },
        "expert": {
            "total_timesteps": 5_000_000,
            "n_envs": 24,
            "n_steps": 2048,
            "batch_size": 2048,
            "learning_rate": 1e-5,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π LR
            "ent_coef": 0.001,  # –ú–µ–Ω—å—à–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            "gamma": 0.999,
            "clip_range": 0.05,  # –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞
            "network_size": [1024, 512, 256, 128],
            "reward_tuning": {  # –î–æ–ø. –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞–≥—Ä–∞–¥
                "checkpoint": 100.0,
                "proximity_coef": 0.8,
            },
            "focus_areas": [],
            "network_migration": {
                "expand_strategy": "depth",
                "weight_init": "orthogonal",
            },
        },
        "master": {
            "total_timesteps": 10_000_000,
            "n_envs": 32,
            "n_steps": 4096,
            "batch_size": 4096,
            "learning_rate": 5e-6,
            "ent_coef": 0.0001,
            "gamma": 0.9995,
            "clip_range": 0.02,
            "network_size": [2048, 1024, 512],
            "reward_tuning": {},
            "focus_areas": [
                (1195, 540),
                (890, 625),
                (760, 215),
                (625, 140),
                (380, 610),
                (395, 195),
                (95, 490),
                (415, 840),
                (850, 860),
                (1085, 940),
                (1360, 780),
                (1460, 755),
            ],  # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–æ–Ω
            "network_migration": {},
        },
    }


def train(stage_name, model=None, vec_normalize=None):
    config = TrainingConfig.STAGES[stage_name]

    # –î–æ–±–∞–≤–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –∫–æ–Ω—Ñ–∏–≥–∞ –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
    def validate_stage_config(config):
        if "network_size" in config:
            for val in config["network_size"]:
                if not isinstance(val, (int, float)):
                    raise TypeError(
                        f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–∏–ø {type(val)} –≤ network_size. –û–∂–∏–¥–∞–µ—Ç—Å—è int/float"
                    )

    validate_stage_config(config)

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–µ–¥—ã
        env = make_vec_env(
            lambda: CarGameEnv(
                track_name="track2",
                render_mode=None,
                reward_config=config["reward_tuning"] or {},
            ),
            n_envs=config["n_envs"],
        )
        env = VecCheckNan(env)

        # –ë–ª–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        vecnorm_path = "./models/vecnormalize.pkl"
        if os.path.exists(vecnorm_path) and vec_normalize is None:
            env = VecNormalize.load(vecnorm_path, env)
            print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ vecnormalize.pkl")
        else:
            env = VecNormalize(env, norm_obs=True, norm_reward=True, training=True)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –Ω–∞–≥—Ä–∞–¥
        if "reward_tuning" in config:
            env.env_method("update_rewards", config["reward_tuning"])

        # –ë–ª–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        model_path = "./models/latest.zip"
        if model is None and os.path.exists(model_path):
            print("‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å")
            model = PPO.load(
                model_path,
                env=env,
                device="cuda",
                custom_objects={
                    "learning_rate": config["learning_rate"],
                    "clip_range": config["clip_range"],
                },
            )
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –∏–∑ {model_path}")

            # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –∫ –Ω–æ–≤—ã–º —Ä–∞–∑–º–µ—Ä–∞–º —Å–µ—Ç–∏
            if "network_size" in config and model is not None:
                print(f"üîÑ –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–µ—Ç–∏ –∫ {config['network_size']}")

                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–ª–∏—Ç–∏–∫–∏
                policy_class = type(model.policy)
                old_policy_kwargs = model.policy_kwargs.copy()

                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–∏–≥—Ä–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
                migration_cfg = config.get(
                    "network_migration",
                    {
                        "expand_strategy": "width",
                        "weight_init": "kaiming",
                        "freeze_existing": False,
                    },
                )

                # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–ª–∏—Ç–∏–∫–∏
                new_policy_kwargs = old_policy_kwargs.copy()
                new_policy_kwargs.update(
                    {
                        "net_arch": dict(
                            pi=config["network_size"], vf=config["network_size"]
                        )
                    }
                )

                # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
                new_model = PPO(
                    env=model.get_env(),
                    policy_kwargs=new_policy_kwargs,
                    device="cuda",
                    **model.get_parameters(),
                )

                # –ü–µ—Ä–µ–Ω–æ—Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                old_params = model.policy.state_dict()
                new_params = new_model.policy.state_dict()

                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                def init_weights(m, method):
                    if method == "kaiming":
                        torch.nn.init.kaiming_normal_(m.weight, nonlinearity="relu")
                    elif method == "orthogonal":
                        torch.nn.init.orthogonal_(m.weight)

                # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤
                for name, param in old_params.items():
                    if name in new_params:
                        new_params[name].copy_(param)
                        if migration_cfg["freeze_existing"]:
                            new_params[name].requires_grad = False

                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤—ã—Ö —Å–ª–æ–µ–≤
                with torch.no_grad():
                    for name, param in new_model.policy.named_parameters():
                        if name not in old_params:
                            print(f"üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ {name}")
                            init_weights(param, migration_cfg["weight_init"])

                new_model.policy.load_state_dict(new_params)

                print("‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —É—Å–ø–µ—à–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞")
                model = new_model

        if model is None:
            policy_kwargs = dict(
                net_arch=dict(pi=config["network_size"], vf=config["network_size"]),
                activation_fn=torch.nn.ReLU,
                ortho_init=True,
            )

            model = PPO(
                "MlpPolicy",
                env,
                verbose=2,
                device="cuda",
                n_steps=config["n_steps"],
                batch_size=config["batch_size"],
                learning_rate=config["learning_rate"],
                ent_coef=config["ent_coef"],
                gamma=config["gamma"],
                clip_range=config["clip_range"],
                vf_coef=0.5,
                max_grad_norm=0.7,
                policy_kwargs=policy_kwargs,
            )

        callbacks = [
            EvalCallback(
                eval_env=env,
                eval_freq=config["total_timesteps"] // 40,
                best_model_save_path="./models/best",
            ),
            ProgressCurriculumCallback(),
            TargetedExploration(config.get("focus_areas", [])),
            AdaptiveCurriculumCallback(config),
        ]

        # –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è
        stage_start = datetime.datetime.now()
        print(f"\n‚è≥ –ù–∞—á–∞–ª–æ —ç—Ç–∞–ø–∞ '{stage_name}' –≤ {stage_start:%Y-%m-%d %H:%M:%S}")

        try:
            model.learn(
                total_timesteps=config["total_timesteps"],
                callback=callbacks,
                reset_num_timesteps=False,
            )

        except Exception as e:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
            error_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            error_model_path = f"./models/crash_{stage_name}_{error_time}.zip"
            error_vecnorm_path = f"./models/crash_vecnorm_{error_time}.pkl"

            print(f"\n‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ '{stage_name}': {str(e)}")
            print(f"üíæ –≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ {error_model_path}")
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ {error_vecnorm_path}")

            model.save(error_model_path)
            env.save(error_vecnorm_path)
            raise  # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–∞–ª—å—à–µ

        finally:
            # –§–∏–Ω–∞–ª–∏–∑–∏—Ä—É—é—â–∞—è —á–∞—Å—Ç—å –≤—Å–µ–≥–¥–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è
            stage_end = datetime.datetime.now()
            duration = stage_end - stage_start
            print(f"\n‚úÖ –≠—Ç–∞–ø '{stage_name}' –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {duration}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            model.save("./models/latest")
            env.save("./models/vecnormalize.pkl")
            print(f"üíæ –ú–æ–¥–µ–ª—å –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ models")

    except Exception as e:
        # –ì–ª–æ–±–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
        print(f"\nüî• –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        print("üîÑ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å n_envs –∏–ª–∏ batch_size")
        raise

    return model, env


if __name__ == "__main__":
    # –≠—Ç–∞–ø—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π
    model, vec_norm = train("initial")
    model, vec_norm = train("medium", model, vec_norm)
    model, vec_norm = train("final", model, vec_norm)
    model, vec_norm = train("expert", model, vec_norm)
    model, vec_norm = train("master", model, vec_norm)

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    model.save("./models/final_model")
    vec_norm.save("./models/final_vecnormalize.pkl")
